{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846fe952",
   "metadata": {},
   "source": [
    "## LANGCHAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519cdfd",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75ed14c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader,PyPDFLoader,WebBaseLoader,ArxivLoader,WikipediaLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93243af9",
   "metadata": {},
   "source": [
    "## STEP-1: DATA INGESTION USING DOCUMENT LOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4635409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'document.txt'}, page_content='Off-spin bowling is one of the most subtle and intellectually demanding arts in the game of cricket. Unlike fast bowling, which relies on pace and physical force, off-spin is built on control, deception, and deep tactical awareness. An off-spinner aims to outthink the batter, using flight, turn, drift, and variations in pace to create opportunities for dismissal. Though it may appear gentle compared to express pace, off-spin has been responsible for some of the greatest moments and match-winning performances in cricket history.\\n\\nOff-spin is delivered by a right-arm bowler who imparts spin on the ball using the fingers, causing it to turn from the off side to the leg side when bowling to a right-handed batter. This direction of spin is known as “off-break.” The bowler typically uses the index finger as the main source of rotation, rolling it down the side of the ball at release. The grip is crucial: the ball rests lightly in the fingers rather than deep in the palm, allowing maximum revolutions and better control. While the physical action is important, the true mastery of off-spin lies in how the bowler uses variations and reads the batter’s intentions.\\n\\nFlight is one of the most important weapons in an off-spinner’s arsenal. By tossing the ball higher in the air, the bowler tempts the batter into playing an attacking shot. The increased loop allows gravity and spin to act on the ball for longer, increasing the chance of turn or drift. Skilled off-spinners know when to give the ball air and when to flatten their trajectory, constantly adjusting based on pitch conditions and the batter’s mindset. A well-flighted delivery can lead to mistimed shots, resulting in catches in the deep or close to the bat.\\n\\nDrift adds another layer of deception. Caused by sidespin and air resistance, drift makes the ball move laterally in the air before pitching. To a batter, the ball may appear to be heading wide of off stump, only to drift back in and turn sharply after bouncing. This movement can upset footwork and force the batter into poor shot selection. Great off-spinners use drift subtly, making the batter doubt their judgment and positioning at the crease.\\n\\nVariation is the hallmark of a successful off-spinner. While the stock off-break remains the foundation, modern off-spin includes deliveries such as the arm ball, which goes straight on with the arm instead of turning. The arm ball is particularly effective against batters who are expecting spin, as it can trap them lbw or bowl them through the gate. Other variations include changes in pace, overspin for extra bounce, and the occasional quicker delivery to surprise the batter. The key is disguise—delivering each variation with the same action so the batter cannot detect it early.\\n\\nOff-spin bowling is also heavily influenced by match conditions. Dry, worn pitches tend to assist spinners, offering more grip and turn as the game progresses. However, skilled off-spinners can still be effective on flatter surfaces by relying on accuracy and patience. Bowling to a disciplined field setting is essential, as off-spinners often aim to build pressure through dot balls rather than sheer aggression. Close-in fielders such as slips, short leg, and silly point play a vital role, ready to capitalize on bat-pad chances and edges.\\n\\nMental strength is perhaps the most underrated aspect of off-spin bowling. Spinners are often targeted by batters looking to score quickly, especially in limited-overs formats. An off-spinner must remain calm under pressure, willing to be hit for boundaries while sticking to a plan. Confidence in one’s skill and strategy is essential, as hesitation can lead to poor execution. The best off-spinners embrace the challenge, viewing aggressive batters as opportunities rather than threats.\\n\\nIn conclusion, off-spin bowling is a craft that blends technique, intelligence, and psychological resilience. It rewards patience, creativity, and a deep understanding of the game. While it may not always grab headlines like fast bowling or explosive batting, off-spin remains a vital and beautiful component of cricket. Those who master it gain not only wickets, but also the respect of teammates and opponents alike.\\n')]\n"
     ]
    }
   ],
   "source": [
    "# text loader\n",
    "loader_text = TextLoader('document.txt')\n",
    "text_document = loader_text.load()\n",
    "print(text_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d2e1a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'Skia/PDF m114 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'I.I.M.U.N. Rules of Procedure.docx', 'source': 'document.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content=\"INDIA’SINTERNATIONALMOVEMENTTOUNITENATIONS\\n1. Applicability: Thebelow-mentionedrulesapplyto alltheIndianCommittees,InternationalBody/organisationSimulations,OtherGovernmentalSimulations,Fictionaland CrisisSimulations,HistoricalandFuturisticSimulations,SpecialCommitteesandanyothercommitteewhichis a partof India'sInternationalMovementto UniteNationsoranyofitsaffiliatedconferencesunlessspecificinstructionshavebeenprovidedforintherules.AnydeviationfromtheRulesofProcedurerequireswrittenpermissionwiththe2/3rdmajorityoftheCoreCouncilandtheassentoftheFounder/PresidentoftheOrganisation.\"), Document(metadata={'producer': 'Skia/PDF m114 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'I.I.M.U.N. Rules of Procedure.docx', 'source': 'document.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='STANDARDRULES\\n2. Representation:Allcouncilsshallhavedelegates/’s representingonecountry/portfoliohavingonevote.Non-governmentalOrganisations& SpecialObserverswillbeallowedto voteonWorkingPapers,AmendmentsandResolutionsthatpertaintothem,idest,inanagendadirectlyconcerningthem.Theywill be allowedto voteon all issuesexceptWorkingPapers,AmendmentsandResolutionsonallotheragendas.ThePresidingOfficer’s decisionshallbefinalandbindinginadispute.\\n3. Language: Theofficiallanguagefor theconferenceshallbe Englishor Hindi.However,committeescanbeconductedinlocal/regionallanguagesifwrittennoticeissentatleast21daysbeforetheeventto theSubstanceDepartmentoftheorganisation.Incaseofanyambiguityregardingthelanguage(or its usage),delegatescantakeclarificationfromtheSubstanceDepartmentoftheorganisation.\\n4. Attire:DelegatesandthePresidingOfficer(s)areexpectedtoweareitherWesternorTraditionalFormalspertheconferenceitinerary.\\n5. UseofElectronicEquipment:Laptops,notepads,andotherelectronicdevicesareallowedduringcommitteetime.However, arestrictiononthiscanbeimposedbythePresidingOfficer.\\n6. Sessions:Eachcommitteesessionshallbe3hoursatmaximum,followedbyamandatorybreakforaminimumof10minutes.TheConferenceshallhaveaminimumof4and12suchsessions.\\nPOWERSOFPRESIDINGOFFICERS&ORGANISINGCOUNCIL\\n7. PresidingOfficer(s): ThedecisionofthePresidingOfficeronmattersregardingdebateisfinalandbindingonalldelegates.Supposearepresentativeisnotbehavinginaparliamentaryfashion.Inthatcase,theycanbebarredfromthecouncilafterbeinggiventhreewarnings.ThePresidingOfficer(s)canquestionanydelegateandaskabouttheirstandonaparticularissue,whichcouldhelpbetterenlightenthecommittee.However, delegatescanappealagainsta decisionofthePresidingOfficer(s)(Rule8),andthepresidingofficercanalsoimpeachthem(Rule9).\\n8. Appeal:If delegatesaredissatisfiedwithhowthePresidingOfficerconductsthecommittee,theycanappealtothePresidingOfficer. Afterbeingrecognisedviaa‘PermissiontoappealtothePresidingOfficer’, thedelegatecanvoicetheirgrievancetothePresidingOfficerwithinaminute’s timeframe,afterwhichthePresidingOfficermayormaynottakeintoconsiderationthedelegate’srecommendation.IffurtherdissatisfiedwiththeverdictofthePresidingOfficer(s),adelegatecanraiseapleatofollowup.Amaximumoftwopleaswillbeallowed,whichmayormaynotberecognised.\\n9. Impeachment:If thedelegatesin thecommitteearehighlydissatisfiedwiththewaythePresidingOfficerisconductingproceedings,delegatescansubmitawrittenrequestsignedbyatleast2/3rd’s ofthecommittee’s membersto theCharged’AffairesoftheConferencewhosedecisionshallbefinalandbinding.'), Document(metadata={'producer': 'Skia/PDF m114 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'I.I.M.U.N. Rules of Procedure.docx', 'source': 'document.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='10.Charged’Affaires:Charged’Affairesoftheconferenceistheheadoftheconference.Supposeanydelegateorobserverisfoundtobemisbehaving.Inthatcase,theycanbebarredfromtheconferenceforanentiredayorforthefullconferencebytheCharged’Affaires,whosedecisionon suchmattersis finalandbinding.TheCharge d’Affairesalsohavesweepingpowersbestoweduponthem.Theirdecisionisfinalandbindinginallmattersconcerningdebate.\\n11.Core Council:Ifthereisa disputeregardingtheverdictgivenbytheCharged’Affaires,thendelegatescanmakeawrittenrequesttotheCoreCouncil.TherequestshouldbecourieredtotheI.I.M.U.N.H.Q.,Mumbai,withinsevendaysoftheevent.\\nRULESREGARDINGDEBATE\\nA)NORMALDEBATE\\n12.Op-ed:Op-edsare a precursorto thecommitteesessions.Thispaperworkcomprisesthedelegate/’s solutionor theframeworkfordealingwiththeagenda.It hasa specificformat[AnnexureI(A)].It maynotnecessarilybeintandemwiththeportfolio(s)allotted,butit mustaddressthequestionathand.Theyhavetobepreparedbythedelegatesbeforethecommitteecanbeginitsproceedings(preparedbeforetheconferencestarts).TheyaretobesubmittedtothePresidingOfficerduringthefirstcommitteesession,andtheminimumnumberofOp-edtobesubmittedis thesameas theQuorumof thecouncil.Supposetheprerequisitenumberofsubmissionsisnotmet.Inthatcase,thedelegatesprepareandsubmittheOp-edsbeforestartingtheformalcommitteesession.\\n13.Quorum: Theminimumnumberofmembersrequiredtobepresentinthecounciltobeginitsproceedings.TheQuorumshouldbe1/5thofthetotalcommitteestrength.\\n14.SettinganAgenda:AcommitteeshalldecideonSettinganAgendainthefollowingmanner:\\na. ThePresidingOfficershallcallforanyPointsorMotionsonthefloor. Tothis,adelegatecanraiseamotiontosettheagendaforaparticulartopicarea.b. ThePresidingOfficershallcall(only)forthosewhoareagainstthemotion.Ifthereisanyopposition,thereshallbeOneFor, OneAgainstthetopicarea,toamaximumofoneminuteeach.c. TheOneFor, OneAgainstmotionshallbefollowedbyaninformalvotewithasimplemajorityforthemotiontopass.Thesecondtopicareaisautomaticallysetfordiscussionif themotionfails.ThecouncilmovesintoanEmergencyDebateifthereisnosecondagenda.\\n15.Speakers’List:A Speakers’Listis automaticallyopenedandremainsopenthroughoutthediscussionofthetopicareaunlessthecommitteechoosestomoveintoaModeratedCaucusoranUnmoderatedCaucus.ASpeakers’Listiswhereadelegateexpressestheircountry’sstandonthetopicarea.Thetimelimitissettooneandahalfminutesperdelegate.Atleasttenspeakersorthecommitteesize,whicheverislower, shouldbeexhaustedalphabeticallyforthefloortoopentootherMotions.'), Document(metadata={'producer': 'Skia/PDF m114 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'I.I.M.U.N. Rules of Procedure.docx', 'source': 'document.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content=\"16.Yields: UtilizedintheSpeakersListattheendofaspeech.Yieldscannotbeexercisedduringanyotherpartofthedebate.Yieldsarecompulsory, anda delegatehasthefollowingwaysofyielding:\\na. YieldtoAnotherDelegate:Thisonlyappliesifthedelegatehasmorethan15secondsremaining.Theycangivetheirremainingtimeto anotherrepresentativeaftertakingwrittenapprovalfromthedelegate.b. Yieldto Questions: A delegatecanYieldto Questions;thecommitteecanaskamaximumoftwoQuestions,anda maximumoftwofollow-upsperquestioncanbeentertainedwiththepermissionofthePresidingOfficer. PresidingOfficer(s)canaskquestionstothedelegates.c. Yieldtocomments: ThePresidingOfficerrecognisesuptotwo30-secondcommentsonthespeechmadebytheSpeaker.\\n17.ModeratedCaucus:A ModeratedCaucusis a formalformof debatewhereina detaileddiscussiononasub-topicofthemainagendahappens.ThemainreasonforstartingaModeratedCaucusis to knowthecommittee’s opiniononsuchanissue.A delegatecanMotionforaModeratedCaucusin thefollowingmanner:‘Motiontoopena ModeratedCaucustodiscuss[Sub-topic]forthetotaltimeduration[max.15minutes]andtimeperspeaker[max.1minute]’.AMotionforaModeratedCaucusrequiresaSimpleMajoritytopassandcanbepassedthroughanInformalVote.Incaseofmultiplemotions,with5beingthemaximum,thePresidingOfficershalldecideonwhichonetoputtothevotefirst.AModeratedCaucuscanbestartedinthemiddleoftheSpeakers’list,idest;itisnotcompulsoryforthedelegatestofinishhearingallthespeakersonthelist,providedthefloorisopen.PresidingOfficersshouldrecognizespeakersatbestinlotsoftwoonly.\\n18.UnmoderatedCaucus:AnUnmoderatedCaucusisaninformalformofdebateandisraisedwhenthecommitteefeelstheneedto discussordecideupona particularissueinformally.DelegatescanalsouseittoformulateWorkingPapersandResolutions.AdelegatecancallforanUnmoderatedCaucusinthefollowingmanner:MotiontoopenanUnmoderatedCaucusfor[Purpose]forthetimeduration[max.15+5minutes]’.AMotionforanUnmoderatedCaucusrequiresaSimpleMajoritytopass.ItcanbepassedthroughanInformalVote.\\n19.Points:Duringthedebate,thefollowingpointsarein orderfrommostdisruptiveto leastdisruptive:\\na. PointofPersonalPrivilege: Thisistheonlypointthatcaninterrupta speakerandisusedwhena delegatecannothearthespeakeroris experiencingmentalorphysicaldiscomfortandwishestobeexcusedfromthecommittee.\\nb. PointofInformation: Thishelpsclarifyanyfactualinaccuracyinadelegate'sspeech.Supposeonefindsthatanotherdelegatehasmisstatedaparticularfactaftercompletingthedelegate'sspeech.Inthatcase,onemayraisetheirplacardand,onbeingrecognised,mayaskforvaliddocumentationsupportingthedelegate'sfacts.However, thispointcannotbeusedtoquestionachangeinthecountry'spolicy. Theaccepteddocumentationsourceincludesbut is not limitedto Reuters,AmnestyInternational,GovernmentWebsitesandanyothercrediblesourceapprovedbythePresidingOfficer(s).ForIndian/RegionalBodies,theacceptedsourcedependsonthecommittee'sPresidingOfficer. ForFictionalCommittees-booksoranydocumentationbasisonwhichthecommitteehasbeenconstrued.\"), Document(metadata={'producer': 'Skia/PDF m114 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'I.I.M.U.N. Rules of Procedure.docx', 'source': 'document.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content=\"c. PointofClarification: Adelegatecanraisethiswhenevertheyhaveaquestionorwouldlikeclarificationfromthedelegatewhohasjustspoken.Thequestioncanhaveuptotwofollow-ups,requiringapprovalfromthePresidingOfficer.\\nd. Pointof Order: ThispointsouttheproceduralinconsistencyordeviationfromtheexistingRulesofProcedure.\\ne. Pointof ParliamentaryInquiry: Thisis exercisedwhena delegatewantsto knowanythingabouttheRulesofProcedurefromthePresidingOfficer(s).\\n20.WorkingPapers:WorkingPapersarethepaperworkwiththecommittee'sviewsonaparticularsub-topicofthemainagenda.Theyprovidedirectiontothecommitteeandindicatehowthecommitteeisflowing.AWorkingPaperhasaprescribedformat(AnnexureI(B))andneedsnosignatories.Still,thefollowingneedsto be incorporated:'Authors',Sub-topic,Number(IIMUN/[Year]/[Chapter_Name]/[Committee_Name]/WP/[0000]).Beforetheintroductionto thecommittee,aWorkingPaperneedstobeapprovedbythePresidingOfficer. AWorkingPaperneedsaSimpleMajoritytobeintroducedonthecommittee'sfloor, andvotingcanbecarriedoutinformally. Inthecaseofmultipleworkingpapers,thePresidingOfficercandecidetheorderinwhichtheyareputto thevote.If a WorkingPaperis passed,it hasto be incorporatedin theresolutioncompulsorily. ThevotingonaWorkingPaperisdoneinformally.\\n21.PresidentialAddress:A delegate,duringthecourseofthedebate,canmakea PresidentialAddress,idest,theHeadofStateisaddressingthecouncil.Thiswillhelpthemexpresstheirstandona particularissue.Delegatescanutilizethisspeechtochangetheirforeignpolicysolongastheythinkthiswillpositivelyimpactsocietyandthediscussedproblem.AcompulsoryYieldtoquestions(Max.2 Questions)willfollowthespeech.Delegatescandothisanytimeduringthedebate,butamaximumof3willbeallowedthroughouttheconference.Thereisnotimelimit,butthePresidingOfficer(s)canimposethesame.\\n22.Resolution:A Resolutionis a paperworkthatcontainsthesolutionto theentireagenda.AResolution,beforebeingintroduced,requiresthepriorapprovalandsignatureofthePresidingOfficer. Thevotingonintroducingtheresolutionfordebatecanbedoneinformally. However,thevoteregardingthepassingof theresolutionmustbe donethroughtheFormalVotingprocedure.TheresolutionmustbeaccordingtotheprescribedformatmentionedinAnnexureI(C).If multipleresolutionsarebroughttothePresidingOfficersimultaneously, theonewithmoresignatoriesshallbeputtothecommitteefirstforvoting.\\ni. Authors: AnAuthoris a personwhowrites,structuresandideatesovertheresolution.Theyhaveto abideby theresolution.A minimumof1 authorisrequiredfortheresolution,andthereis nolimitonthemaximum,subjecttofulfillingthecriterionofsignatories,idest,andcommitteestrengthwithoutthesignatories.Anauthorcannotvoteagainsttheresolution.However, anauthorcanwithdrawaresolution.\"), Document(metadata={'producer': 'Skia/PDF m114 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'I.I.M.U.N. Rules of Procedure.docx', 'source': 'document.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='ii. Signatories: Signatoriesconsentonlytodiscusstheresolutioninthecouncilandcanexercisetheirvotingrightsthewaytheyplease.Aminimumof3signatoriesarerequiredfortheresolution.\\n23.Amendments:AllamendmentsmustbewrittenandsubmittedtothePresidingOfficer(s).Theformatformodificationsshouldinclude:Authors,Type(Add,DeleteorReplace),FormandtheClausewithitsnumber(AnnexureI (E)).TherearetwoformsofAmendments,whichcanberaisedby raisinga MotionforAmendmentandfollowedby theapprovalofthePresidingOfficer:i. FriendlyAmendments: Anamendmentagreeduponbyalltheauthors/’s doesnotrequirevoting.ii. Amendments:Amendmentsintroducedbyanyotherneedtobevoteduponbythecounciltobeincorporatedintheresolution.ASimpleMajorityisrequiredtointroduceanAmendment,followedbyDiscussionandInformalVoting.\\n24. E-mendments:AnycorrectionofgrammaticalerrorsordeviationfromtheprescribedwritingformatiscalledanE-mendmentanddoesnotrequireanyvotingbuta simpleapprovalofthePresidingOfficer.\\nB) EMERGENCYDEBATE\\nDuringtheemergencydebateat I.I.M.U.N.(or its affiliatedconferences),an emergency’smaximumdurationshouldbe 3 hours.Noobserversshouldbe allowedon thecommittee.Delegatesarenotallowedto leavethecommitteeduringtheEmergencyDebate.A detailedintroductionto the crisisis a must.PresidentialstatementsarenotpermittedduringtheEmergency.\\n25.Quorum:Atleast1/5thofthemembercountriesinvolvedintheemergencyshouldbepresent.\\n26.RecognisingSpeakers:Incaseofanemergencysession,therewillbenoSpeakers’List,andthePresidingOfficershallrecogniseaminimumof5individuals,intheorderofinvolvementinthecrises,tospeakforaperiodofaminuteandahalfeach.\\n27.ModeratedCaucus:ThePresidingOfficershalldecideuponthetopicsfortheModeratedCaucus,thetimeframeandthetimeperspeaker. Amaximumoffivemoderatedcaucusesareallowed.\\n28.Points:AllpointsexceptforthePointofPersonalPrivilegeshallbesuspendedforthedurationoftheEmergency.\\n29.UnmoderatedCaucus:AtleastoneUnmoderatedCaucusshouldbeutilisedinformulatingtheDeclaration.\\n30.Declaration:Thiscombinesa WorkingPaperanda Resolutionbutis easierandfaster. ADeclarationissimilartotheResolutionbutdoesnothaveIntroductoryClauses.TheformatofaDeclarationismentionedinAnnexureI(D).Itrequires1‘Author’ and3Signatoriesapartfrompriorapprovalfromthe PresidingOfficer. AnInformalVoteis enoughto decidewhichdeclarationtodiscuss,andaspecialmajorityisrequiredtopassadeclaration.'), Document(metadata={'producer': 'Skia/PDF m114 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'I.I.M.U.N. Rules of Procedure.docx', 'source': 'document.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='C)\\nRULESCOMMONTOBOTHFORMSOFDEBATE\\n31.Challenge:Thisisaninformalbilateraltalkonamultilateralforum.Itisraisedwhenadelegatewantstochallengeanotherdelegateoveraparticularissueforaspecifiedtime;thismaybeovera specificsub-topicorportfoliopolicy. Itcanalsobeusedasadisputeresolutiontechniquebydelegates.Thisisaone-on-onedebateandrequiresonlyapprovalfromthePresidingOfficerandthedelegatethathasbeenchallenged.AdelegatemayMotionforit inthefollowingmanner:‘MotiontoChallengethedelegate/delegationof[Country]over[topicarea]foratimedurationof[Max.3minutes].’\\n32.PrivateBilateralTalk:Thisisaprivateconversationbetweentwocountries/delegationsandisusedforsolving/settlinganycontentionsviainformalnegotiations.Delegatescanraisethisatanytimeduringthedebate,butamaximumof3suchmotionsperdelegationareallowed.Itonlyrequirestheapprovalof thePresidingOfficer, providedall thedelegatesinvolvedareinconsensus.Delegatescanonlymoveintothisbymakinga writtenrequestto thePresidingOfficer. Ifgranted,thedelegatescanleavethecommitteeforamaximumof10minutes.ThesaidtalkcanalsoincludearbitrationbythePresidingOfficer(s),providedallthepartiesagreeandfeeltheneedforthesame.\\n33.QuestionAnswerSession:Thisiswhenmanydelegateswishtoaskquestionstoadelegate/‘s.Anydelegatemaycallforthesame.Ifpassedbyaninformalcommitteevoteandagreeduponbythedelegates/‘sbeingquestioned,thenthecommitteecanaskamaximumofupto10questionstothatdelegate/‘s.Amaximumof2delegatesareallowedtobeonthedaistoanswerquestionsposedbythecommittee.To callforthesame,a delegatecanMotion:‘MotiontomoveintoaQuestionAnswerSessionwiththedelegateof[Country],forthetotalnumberof[Maxof10Questions]Questions’.\\n34.OneForOneAgainstThisiswhenthecommitteeusuallycannotdecideonaparticularissueorisdividedonthequestion.Therefore,beforethefinalvote,oneneedstohearbothsidesoftheargument.ThiscanbeextendedtoaTwoForandTwoAgainst.TheTimeLimitforeachspeakeris1minute.\\n35.FormalVoting:ThisisrequiredonlyforpassingaResolutionoraDeclarationandtakesplacein threerounds.NoobserverotherthanthosewhohavebeenbestowedtheRightto Vote,memberof thePressor theAdministrationstaff is allowedto bepresentduringvoting.AresolutionpassesifithastheSpecialMajority. Thethreeroundsofvotingare:\\nRound1:Alldelegateshaveanoptionbetweenchoosing:a. Yesb. Noc. YeswithRightsd. NowithRightse. Passf. Abstain\\nRound2:  Alldelegatesthathaveoptedforrightsgettojustifytheirpositions.ThisroundisenteredonlyifthereisaYeswithRightsoraNowithRightsandaPass.\\nRound3:Delegatesmustcasttheirfinalvote,andtheirthreechoicesare:aYes,aNoandanAbstain.'), Document(metadata={'producer': 'Skia/PDF m114 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'I.I.M.U.N. Rules of Procedure.docx', 'source': 'document.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='36.Clause-by-ClauseVoting:ThisappliesonlytotheResolutions,nottheDeclarations.Adelegatemaymotionforthis,andinformalvotingshalloccurforeveryclause.ASimpleMajorityisrequiredforeachclausetopass.However, thefullresolutionshallbeputtothevotethroughtheFormalVotingprocedure.\\n37.Majority:\\nA. SimpleMajority:Asimplemajorityismetwhen50%+1ofthememberspresentinthecommitteevoteinfavourofamotionorpaperwork.\\nB. SpecialMajority: Aspecialmajorityismetwhentwo-thirdsofthememberspresentinthecommitteevoteinfavourofamotionorpaperwork.\\n38.InformalVoting:Informalvotingisrequiredtopassvariousmotionsduringthedebate.It isconductedsimplybyaskingfortheonesinfavourandcountingthenumberofplacardsraised,followedbycountingtheonesagainst.\\n39.TablingofPaperwork:Thedelegatesofthecouncilhavea choiceto‘table’anyofthethreementioneddocuments,idest,WorkingPaper, Resolution,andDeclaration,atanypointoftimeduringtheirdeliberationincouncilforwhichtheyrequireaSpecialMajority, InformalVote.ThetablingofPaperworkmeansthatthesamecannotbediscussedagain.\\n40.JointStatement:Twoormorecountriescancometogetherandmakea jointannouncementregardingtheirviewsonthetopicareas.Delegatescanutilizethistochangetheirforeignpolicysolongastheythinkthiswillpositivelyimpactsocietyandtheproblembeingdiscussedatthetime.Delegatescanmakea jointstatementatanytimeduringthecommittee.ThePresidingOfficer(s)mustapproveajointstatementbeforebeingintroducedtothecommittee.Thisisinawrittenformat(AnnexureI(F).\\n41.Censure:Thedelegatescancensureafellowdelegatebymotioningforthesame,butforittobesuccessful,it hasto havea SpecialMajoritythroughaninformalvote.ThisMotioncanbeoverruledatthediscretionofthePresidingOfficer.'), Document(metadata={'producer': 'Skia/PDF m114 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'I.I.M.U.N. Rules of Procedure.docx', 'source': 'document.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='ANNEXUREI\\nFORMATS\\n1. OP-EDFORMAT\\nNUMBER:IIMUN/2023/[Chapter_Name]/[Committee_Name]/OE/0000PORTFOLIO/COUNTRY:[1Max.]\\n1. Point12. Point23. Point3\\nOr\\nParagraph1Paragraph2Paragraph3\\n2. WORKINGPAPERFORMAT\\nNUMBER:IIMUN/2023/[Conference_Name]/[Committee_Name]/WP/0000AUTHOR(s):[1Min.]SUB-TOPIC:XYZ\\n1.Point12.Point23.Point3\\n3. RESOLUTIONFORMAT\\nNUMBER:IIMUN/2023/[Conference_Name]/[Committee_Name]/RES/0000AUTHOR(s):[1Min.]SIGNATORIES:[3Min.]AGENDA:XYZ\\n1.IntroductoryClause1.2.IntroductoryClause2.3.IntroductoryClause3.\\n(continued)'), Document(metadata={'producer': 'Skia/PDF m114 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'I.I.M.U.N. Rules of Procedure.docx', 'source': 'document.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='The[Committee_Name_FullForm],1. ActionableClause1.2. ActionableClause2.3. ActionableClause3.\\n4. DECLARATIONFORMAT\\nNUMBER:IIMUN/2023/[Conference_Name]/[Committee_Name]/DEC/0000AUTHOR(s):[1Min.]SIGNATORIES:[3Min.]AGENDA:XYZ\\nThe[Committee_Name_FullForm],1. ActionableClause1.2. ActionableClause2.3. ActionableClause3.\\n5. AMENDMENTSFORMAT\\nAUTHOR(s):[1Min.]TYPE:Add/Delete/ReplaceFORM:FriendlyAmendmentorAmendmentCLAUSE:\\n6. JOINTSTATEMENT\\nAUTHORS:[Min2.]TOPIC:\\n1. PointA.2. PointB.3. PointC.'), Document(metadata={'producer': 'Skia/PDF m114 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'I.I.M.U.N. Rules of Procedure.docx', 'source': 'document.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content=\"ANNEXUREII\\nLISTOFCLAUSES\\n1) IntroductoryClauses:Theseclausesareanintroductionto theresolutionorsolutionandoftenreferto pastresolutions,citationsofspeechesmade.Afewoftheintroductoryphrasesarelistedhere:Affirming,Alarmedby, Approving,Awareof,Bearingin mind,Believing,Confident,Contemplating,Convinced,Declaring,Deeplyconcerned,Deeplyconscious,Deeplyconvinced,Deeplydisturbed,Deeplyregretting,DesiringEmphasizing,Expecting,Expressingits appreciation,Expressingitssatisfaction,Fulfilling,Fullyalarmed,Fullyaware,Fullybelieving,Furtherdeploring,Furtherrecalling,Guidedby, Havingadopted,Havingconsidered,Havingconsideredfurther, Havingdevotedattention,Havingexamined,Havingheard,HavingreceivedHavingstudied,Keepinginmind,Notingwithregret,Notingwithdeepconcern,Notingwithsatisfaction,Notingfurther, Notingwithapproval,Observing,Reaffirming,Realizing,Recalling,Recognizing,Referring,Seeking,Takingintoaccount,Takingintoconsideration,Takingnote,Viewingwithappreciation,Welcoming\\n2) ActionableClauses:Actionableclausescontaintheactualsolutiontotheagendaorcrisis.Afewoftheactionablephrasesarelistedhere:Accepts,Affirms,Approves,Authorizes,Calls,Callsupon,Condemns,Confirms,Congratulates,Considers,Declaresaccordingly, Deplores,Designates,Drawsthe attention,Emphasizes,Encourages,Endorses,Expressesits appreciation,Expressesits hope,Furtherinvites,Furtherproclaims,Furtherreminds,Furtherrecommends,Furtherrequests,Furtherresolves,Hasresolved,Notes,Proclaims,Reaffirms,Recommends,Regrets,Reminds,Requests,Solemnlyaffirms,Stronglycondemns,Supports,Takesnoteof,Transmits,Trusts\\nTheofficialrulesofprocedureofIndia'sInternationalMovementtoUniteNationsaretheorganisation'sproperty.Anycopy,re-printing,orusagemustbedonewithexpresswrittenpermissionfromtheorganisation.Forfurtherdetails,contactsubstancedept@gmail.com.\\nIndia'sInternationalMovementtoUniteNations\")]\n"
     ]
    }
   ],
   "source": [
    "# pdf loader\n",
    "loader_pdf = PyPDFLoader('document.pdf')\n",
    "pdf_document = loader_pdf.load()\n",
    "print(pdf_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d8d23f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://en.wikipedia.org/wiki/Agentic_AI', 'title': 'Agentic AI - Wikipedia', 'language': 'en'}, page_content='\\n\\n\\n\\nAgentic AI - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact us\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDonate\\n\\nCreate account\\n\\nLog in\\n\\n\\n\\n\\n\\n\\n\\n\\nPersonal tools\\n\\n\\n\\n\\n\\nDonate Create account Log in\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n(Top)\\n\\n\\n\\n\\n\\n1\\nOverview\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\nHistory\\n\\n\\n\\n\\n\\n\\n\\n\\n3\\nTraining and testing\\n\\n\\n\\n\\n\\n\\n\\n\\n4\\nAutonomous capabilities\\n\\n\\n\\n\\n\\n\\n\\n\\n5\\nArchitectural patterns\\n\\n\\n\\n\\n\\n\\n\\n\\n6\\nMultimodal AI agents\\n\\n\\n\\n\\n\\n\\n\\n\\n7\\nApplications\\n\\n\\n\\n\\nToggle Applications subsection\\n\\n\\n\\n\\n\\n7.1\\nWeb browsing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n8\\nProposed benefits\\n\\n\\n\\n\\n\\n\\n\\n\\n9\\nConcerns\\n\\n\\n\\n\\nToggle Concerns subsection\\n\\n\\n\\n\\n\\n9.1\\nPossible mitigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n10\\nSee also\\n\\n\\n\\n\\n\\n\\n\\n\\n11\\nReferences\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nAgentic AI\\n\\n\\n\\n6 languages\\n\\n\\n\\n\\nCatalàČeštinaفارسی한국어ՀայերենРусский\\n\\nEdit links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\t\\tGeneral\\n\\t\\n\\n\\nWhat links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code\\n\\n\\n\\n\\n\\n\\t\\tPrint/export\\n\\t\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\n\\t\\tIn other projects\\n\\t\\n\\n\\nWikidata item\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n\\nSystems that perform tasks without human intervention\\nA request that this article title be changed\\xa0to AI agentAI agent is under discussion. Please do not move this article until the discussion is closed.\\n\\nIn the context of generative artificial intelligence, AI agents (also referred to as compound AI systems or agentic AI) are a class of intelligent agents distinguished by their ability to operate autonomously in complex environments. Agentic AI tools prioritize decision-making over content creation and do not require human prompts or continuous oversight.[1]\\n\\n\\nOverview[edit]\\nAI agents possess several key attributes, including complex goal structures, natural language interfaces, the capacity to act independently of user supervision, and the integration of software tools or planning systems. Their control flow is frequently driven by large language models (LLMs).[2] Agents also include memory systems for remembering previous user-agent interactions and orchestration software for organizing agent components.[3]\\nResearchers and commentators have noted that AI agents do not have a standard definition.[2][4][5][6] The concept of agentic AI has been compared to the fictional character J.A.R.V.I.S..[7]\\nA common application of AI agents is the automation of tasks—for example, booking travel plans based on a user\\'s prompted request.[8][9] Prominent examples include Devin AI, AutoGPT, and SIMA.[10] Further examples of agents released since 2025 include OpenAI Operator,[11] ChatGPT Deep Research,[12] Manus,[13] Quark (based on Qwen),[14] AutoGLM Rumination,[14] and Coze (by ByteDance).[14] Frameworks for building AI agents include LangChain,[15] as well as tools such as CAMEL,[16][17] Microsoft AutoGen,[18] and OpenAI Swarm.[19]\\nCompanies such as Google, Microsoft and Amazon Web Services have offered platforms for deploying pre-built AI agents.[20]\\nProposed protocols for standardizing inter-agent communication include the Agent Protocol (by LangChain), the Model Context Protocol (by Anthropic), AGNTCY,[21] Gibberlink,[22] the Internet of Agents,[23] Agent2Agent (by Google),[24] and the Agent Network Protocol.[25] Some of these protocols are also used for connecting agents with external applications.[3] Software frameworks for addressing agent reliability include AgentSpec, ToolEmu, GuardAgent, Agentic Evaluations, and predictive models from H2O.ai.[26]\\nIn February 2025, Hugging Face released Open Deep Research, an open source version of OpenAI Deep Research.[27] Hugging Face also released a free web browser agent, similar to OpenAI Operator.[28] Galileo AI published on Hugging Face a leadership board for agents, which ranks their performance based on their underlying LLMs.[29]\\nMemory systems for agents include Mem0,[30][31] MemGPT,[32] and MemOS.[33]\\n\\nHistory[edit]\\nMain article: History of artificial intelligence\\nAI agents have been traced back to research from the 1990s, with Harvard professor Milind Tambe noting that the definition of an AI agent was not clear at the time either. Researcher Andrew Ng has been credited with spreading the term \"agentic\" to a wider audience in 2024.[34]\\n\\nTraining and testing[edit]\\nResearchers have attempted to build world models[35][36] and reinforcement learning environments[37] to train or evaluate AI agents. For example, video games such as Minecraft[38] and No Man\\'s Sky[39] as well as replicas of company websites,[40] have also been used for training AI agents.\\n\\nAutonomous capabilities[edit]\\nThe Financial Times compared the autonomy of AI agents to the SAE classification of self-driving cars, comparing most applications to level 2 or level 3, with some achieving level 4 in highly specialized circumstances, and level 5 being theoretical.[41]\\n\\nArchitectural patterns[edit]\\nSee also: Large language model §\\xa0Agency\\nCommon architectural design patterns for agents include:\\n\\nRetrieval-augmented generation[42]\\nReAct (Reason + Act),[43] an extension of chain-of-thought prompting that queries the underlying model to explain its reasoning before taking any action.[44]\\nReflexion,[42][43][44] which uses an LLM to create feedback on the agent\\'s plan of action and stores that feedback in a memory cache.\\nA tool/agent registry,[42] for organizing software functions or other agents that the agent can use.\\nOne-shot model querying,[42] which queries the model once to create the plan of action.\\nMultimodal AI agents[edit]\\nIn addition to large language models (LLMs), vision-language models (VLMs) and multimodal foundation models can be used as the basis for agents. In September 2024, Allen Institute for AI released an open-source vision-language model, which Wired noted could give AI agents the ability to perform complex computer tasks, including the possibility of automated computer hacking.[45] Nvidia released a framework for developers to use VLMs, LLMs and retrieval-augmented generation for building AI agents that can analyze images and videos, including video search and video summarization.[46][47] Microsoft released a multimodal agent model – trained on images, video, software user interface interactions, and robotics data – that the company claimed can manipulate software and robots.[48]\\n\\nApplications[edit]\\nAs of April 2025, per the Associated Press, there are few real-world applications of AI agents.[49] As of June 2025, per Fortune, many companies are primarily experimenting with AI agents.[50]\\nA recruiter for the Department of Government Efficiency proposed in April 2025 to use AI agents to automate the work of about 70,000 United States federal government employees, as part of a startup with funding from OpenAI and a partnership agreement with Palantir. This proposal was criticized by experts for its impracticality, if not impossibility, and the lack of corresponding widespread adoption by businesses.[51]\\nThe Information divided AI agents into seven archetypes: business-task agents, for acting within enterprise software; conversational agents, which act as chatbots for customer support; research agents, for querying and analyzing information (such as OpenAI Deep Research); analytics agents, for analyzing data to create reports; software developer or coding agents (such as Cursor); domain-specific agents, which include specific subject matter knowledge; and web browser agents (such as OpenAI Operator).[3]\\nBy mid-2025, AI agents have been used in video game development,[52] gambling (including sports betting),[53] and cryptocurrency wallets[53] (including cryptocurrency trading and meme coins[54]). In August 2025, New York Magazine described software development as the most definitive use case of AI agents.[55] Likewise, by October 2025, noting a decline in expectations, The Information noted AI coding agents and customer support as the primary use cases by businesses.[56]\\nAI agents have also been integrated into operating systems. Writing in The Economist, Signal president Meredith Whittaker has noted that agents have been included in operating systems developed by Microsoft, Apple and Google.[57] In November 2025, Microsoft released a test software build of Windows 11 that included agents intended to run background tasks, with the ability to read and write personal files.[58] In December 2025, ByteDance released Doubao, an AI agent that can be integrated into smartphone operating systems, particularly the Nubia M153 by ZTE.[59] Several apps in China blocked or restricted the agent, citing privacy and security concerns,[60] including WeChat,[59] Alipay, Taobao, Pinduoduo, Ele.me,[61] and local banks.[62]\\nIn November 2025, The Wall Street Journal reported that few companies that deployed AI agents have received a return on investment.[63]\\nSeveral government bodies in the United States and United Kingdom have deployed or announced the deployment of agents. The city of Kyle, Texas deployed an AI agent from Salesforce in March 2025 for 311 customer service.[64] In November 2025, the Internal Revenue Service stated that it would use Agentforce, AI agents from Salesforce, for the Office of Chief Counsel, Taxpayer Advocate Services and the Office of Appeals.[65] That same month, Staffordshire Police announced that they would trial Agentforce agents for handling non-emergency 101 calls in the United Kingdom starting in 2026.[66] In December 2025, the Food and Drug Administration announced that it would offer \"agentic AI capabilities\" to its staff for \"meeting management, pre-market reviews, review validation, post-market surveillance, inspections and compliance and administrative functions.\"[67] That same month, the United States Department of Defense launched GenAI.mil, an internal platform for American military personnel to use generative AI-based applications based on Google Gemini, including \"intelligent agentic workflows\". Defense Secretary Pete Hegseth listed applications such as \"[conducting] deep research, [formatting] documents and even [analyzing] video or imagery at unprecedented speed.\"[68]\\n\\nWeb browsing[edit]\\nWeb browsers with integrated AI agents are sometimes called agentic browsers. Such agents can perform small tedious tasks during web browsing and potentially even perform browser actions on behalf of the user. Products like OpenAI Operator and Perplexity Comet integrate a spectrum of AI capabilities including the ability to browse the web, interact with websites and perform actions on behalf of the user.[69][70] In 2025, Microsoft launched NLWeb, an agentic web search replacement that would allow websites to use agents to query content from websites by using RSS-like interfaces that allow for the lookup and semantic retrieval of content.[71] Products integrating agentic web capabilities have been criticised for exfiltrating information about their users to third-party servers[72] and exposing security issues since the way the agents communicate often occur through non-standard protocols.[71]\\n\\nProposed benefits[edit]\\nProponents argue that AI agents can increase personal and economic productivity,[9][73] foster greater innovation,[74] and liberate users from monotonous tasks.[74][75] A Bloomberg opinion piece by Parmy Olson argued that agents are best suited for narrow, repetitive tasks with low risk.[76] Conversely, researchers suggest that agents could be applied to web accessibility for people who have disabilities,[77][78] and researchers at Hugging Face propose that agents could be used for coordinating resources such as during disaster response.[79] The R&D Advisory Team of the BBC views AI agents as being most useful when their assigned goal is uncertain.[80] Erik Brynjolfsson suggests that AI agents are more valuable enhancing, rather than replacing, humans.[81]\\n\\nConcerns[edit]\\nConcerns include potential issues of liability,[73][80] an increased risk of cybercrime,[8][73] ethical challenges,[73] as well as problems related to AI safety[73] and AI alignment.[8][75] Other issues involve data privacy,[8][82] weakened human oversight,[8][73][79] a lack of guaranteed repeatability,[83] reward hacking,[84] algorithmic bias,[82][85] compounding software errors,[8][10] lack of explainability of agents\\' decisions,[8][86] security vulnerabilities,[8][87] stifling competition,[57] problems with underemployment,[85] job displacement,[9][85] cognitive offloading,[88] and the potential for user manipulation,[86][89] misinformation[79] or malinformation.[79] They may also complicate legal frameworks and risk assessments, foster hallucinations, hinder countermeasures against rogue agents, and suffer from the lack of standardized evaluation methods.[90][8][91] They have also been criticized for being expensive[2][8] and having a negative impact on internet traffic,[8] and potentially on the environment due to high energy usage.[83][92][93] According to an estimation by Nvidia CEO Jensen Huang, AI agents would require 100 times more computing power than LLMs.[94] There is also the risk of increased concentration of power by political leaders, as AI agents may not question instructions in the same way that humans would.[84]\\nJournalists have described AI agents as part of a push by Big Tech companies to \"automate everything\".[95] Several CEOs of those companies have stated in early 2025 that they expect AI agents to eventually \"join the workforce\".[96][97] However, in a preprint study, Carnegie Mellon University researchers tested the behavior of agents in a simulated software company and found that none of the agents could complete a majority of the assigned tasks.[96][98] Other researchers had similar findings with Devin AI[99] and other agents in business settings[100][101] and freelance work.[102] CNN argued that statements by CEOs on the potential replacement of their employees by AI agents were a strategy to \"[keep] workers working by making them afraid of losing their jobs.\"[103] Tech companies have pressured employees to use generative AI models in their work, including AI coding agents. Brian Armstrong, the CEO of Coinbase, fired several employees who did not.[104][105] Some business leaders have replaced some of their employees with agents, but have said that the agents would need more supervision than those employees.[56] Futurism questioned whether Amazon\\'s previously announced efforts to replace parts of its workforce with generative AI and AI agents could have led to the October 2025 outage of Amazon Web Services.[106]\\nYoshua Bengio warned at the 2025 World Economic Forum that \"all of the catastrophic scenarios with AGI or superintelligence happen if we have agents\".[107]\\nIn March 2025, Scale AI signed a contract with the United States Department of Defense to work with them, in collaboration with Anduril Industries and Microsoft, to develop and deploy AI agents for the purpose of assisting the military with \"operational decision-making\".[108] In July 2025, Fox Business reported that the company EdgeRunner AI built an offline agent, compressed and fine-tuned on military information, with the CEO seeing more common LLMs as \"heavily politicized to the left\". As of that time, the company model is being used by the United States Special Operations Command in an overseas deployment.[109] Researchers have expressed concerns that agents and the large language models they are based on could be biased towards aggressive foreign policy decisions.[110][111]\\nResearch-focused agents have the risk of consensus bias and coverage bias due to collecting information available on the public Internet.[112] NY Mag unfavorably compared the user workflow of agent-based web browsers to Amazon Alexa, which was \"software talking to software, not humans talking to software pretending to be humans to use software.\"[113] The same outlet described web browser agents and computer-use agents as an attempt to \"click-farm the entire economy.\"[114]\\nAgents have been linked to the dead Internet theory due to their ability to both publish and engage with online content.[115]\\nAgents may get stuck in infinite loops.[11][116]\\nSince many inter-agent protocols are being developed by large technology companies, there are concerns that those companies could use these protocols for self-benefit.[25]\\nA June 2025 Gartner report accused many projects described as agentic AI of being rebrands of previously released products, terming the phenomenon as \"agent washing\".[55]\\nResearchers have warned about the impact of providing AI agents access to cryptocurrency and smart contracts.[54]\\nDuring a vibe coding experiment, a coding agent by Replit deleted a production database during a code freeze, \"[covered] up bugs and issues by creating fake data [and] fake reports\" and responded with false information.[117][118] A user of Google Antigravity reported that, when the user attempted to use the system to delete cache, the system responded by deleting the user\\'s D hard drive.[119]\\nIn July 2025, PauseAI referred OpenAI to the Australian Federal Police, accusing the company of violating Australian laws through ChatGPT agent due to the risk of assisting the development of biological weapons.[120]\\nIssues with multi-agent systems include few coordination protocols between component agents, inconsistent performance, and challenges debugging.[121]\\nIn November 2025, Anthropic claimed that a group of hackers sponsored by China attempted a cyberattack against at least 30 organizations by using Claude Code in an agentic workflow, and that several of these infiltrations had succeeded.[122] However, independent cybersecurity researchers questioned the significance of Anthropic\\'s findings.[122][123]\\nWhittaker argued that the push by Big Tech companies to deploy AI agents risked security vulnerabilities across the Internet.[124]\\n\\nPossible mitigation[edit]\\nZico Kolter noted the possibility of emergent behavior as a result of interactions between agents, and proposed research in game theory to model the risks of these interactions.[125]\\nGuardrails, defined by Business Insider as \"filters, rules, and tools that can be used to identify and remove inaccurate content\" have been suggested to help reduce errors.[126]\\nTo address security vulnerabilities related to data access, language models could be redesigned to separate instructions and data, or agentic applications could be required to include guardrails. These ideas were proposed in response to a zero-click exploit that affected Microsoft 365 Copilot.[50] Confidential computing has been proposed for protecting data security in projects involving AI agents and generative AI.[127]\\nA pre-print by Nvidia researchers has suggested small language models (SLMs) as an alternative to LLMs for AI agents, arguing that SLMs are cheaper and more energy efficient.[128][129]\\nThe Economist has advised avoiding what Simon Willison has described as the \"lethal trifecta\" for AI agents and LLMs: \"outside-content exposure, private-data access and outside-world communication\".[130]\\n\\nSee also[edit]\\nIntelligent agent\\nModel Context Protocol\\nRational agent\\nRobotic process automation\\nSoftware agent\\nReferences[edit]\\n\\n^ Purdy, Mark (December 12, 2024). \"What Is Agentic AI, and How Will It Change Work?\". Harvard Business Review. ISSN\\xa00017-8012. Retrieved April 24, 2025.\\n\\n^ a b c Kapoor, Sayash; Stroebl, Benedikt; Siegel, Zachary S.; Nadgir, Nitya; Narayanan, Arvind (2024). \"AI Agents That Matter\". arXiv:2407.01502 [cs.LG].\\n\\n^ a b c Holmes, Aaron (July 7, 2025). \"The Seven Kinds of AI Agents\". The Information. Archived from the original on July 20, 2025. Retrieved November 9, 2025.\\n\\n^ Zeff, Maxwell; Wiggers, Kyle (March 14, 2025). \"No one knows what the hell an AI agent is\". TechCrunch. Archived from the original on March 18, 2025. Retrieved May 15, 2025.\\n\\n^ Varanasi, Lakshmi. \"AI agents are all the rage. But no one can agree on what they do\". Business Insider. Archived from the original on April 11, 2025. Retrieved May 15, 2025.\\n\\n^ Bort, Julie (May 12, 2025). \"Even a16z VCs say no one really knows what an AI agent is\". TechCrunch. Archived from the original on May 12, 2025. Retrieved May 15, 2025.\\n\\n^ Field, Hayden (August 31, 2025). \"AI agents are science fiction not yet ready for primetime\". The Verge. Archived from the original on September 15, 2025. Retrieved November 9, 2025.\\n\\n^ a b c d e f g h i j k \"AI Agents: The Next Generation of Artificial Intelligence\". The National Law Review. December 30, 2024. Archived from the original on January 11, 2025. Retrieved January 14, 2025.\\n\\n^ a b c \"What are the risks and benefits of \\'AI agents\\'?\". World Economic Forum. December 16, 2024. Archived from the original on December 28, 2024. Retrieved January 14, 2025.\\n\\n^ a b Knight, Will (March 14, 2024). \"Forget Chatbots. AI Agents Are the Future\". Wired. ISSN\\xa01059-1028. Archived from the original on January 5, 2025. Retrieved January 14, 2025.\\n\\n^ a b Marshall, Matt (February 22, 2025). \"The rise of browser-use agents: Why Convergence\\'s Proxy is beating OpenAI\\'s Operator\". VentureBeat. Archived from the original on February 22, 2025. Retrieved April 2, 2025.\\n\\n^ Milmo, Dan (February 3, 2025). \"OpenAI launches \\'deep research\\' tool that it says can match research analyst\". The Guardian. ISSN\\xa00261-3077. Archived from the original on February 3, 2025. Retrieved April 2, 2025.\\n\\n^ Chen, Caiwei (March 11, 2025). \"Everyone in AI is talking about Manus. We put it to the test\". MIT Technology Review. Archived from the original on March 12, 2025. Retrieved April 2, 2025.\\n\\n^ a b c \"China is gaining ground in the global race to develop AI agents\". Rest of World. June 2, 2025. Archived from the original on June 2, 2025. Retrieved June 12, 2025.\\n\\n^ David, Emilia (December 30, 2024). \"Why 2025 will be the year of AI orchestration\". VentureBeat. Archived from the original on December 30, 2024. Retrieved January 14, 2025.\\n\\n^ \"CAMEL: Finding the Scaling Law of Agents. The first and the best multi-agent framework\". GitHub.\\n\\n^ Li, Guohao (2023). \"Camel: Communicative agents for \"mind\" exploration of large language model society\" (PDF). Advances in Neural Information Processing Systems. 36: 51991–52008. arXiv:2303.17760. S2CID\\xa0257900712.\\n\\n^ Dickson, Ben (October 3, 2023). \"Microsoft\\'s AutoGen framework allows multiple AI agents to talk to each other and complete your tasks\". VentureBeat. Archived from the original on December 27, 2024. Retrieved January 14, 2025.\\n\\n^ \"The next AI wave — agents — should come with warning labels\". Computerworld. January 13, 2025. Archived from the original on January 14, 2025. Retrieved January 14, 2025.\\n\\n^ David, Emilia (April 15, 2025). \"Moveworks joins AI agent library craze\". VentureBeat. Archived from the original on April 15, 2025. Retrieved May 14, 2025.\\n\\n^ David, Emilia (March 6, 2025). \"A standard, open framework for building AI agents is coming from Cisco, LangChain and Galileo\". VentureBeat. Archived from the original on March 9, 2025. Retrieved April 2, 2025.\\n\\n^ Zeff, Maxwell (March 5, 2025). \"GibberLink lets AI agents call each other in robo-language\". TechCrunch. Archived from the original on March 5, 2025. Retrieved April 2, 2025.\\n\\n^ Cooney, Michael (January 30, 2025). \"Cisco touts \\'Internet of Agents\\' for secure AI agent collaboration\". Network World. Archived from the original on January 31, 2025. Retrieved April 2, 2025.\\n\\n^ Clark, Lindsay (April 10, 2025). \"Did someone say AI agents, Google asks, bursting in\". The Register. Archived from the original on April 10, 2025. Retrieved May 14, 2025.\\n\\n^ a b Stokel-Walker, Chris (June 11, 2025). \"Can we stop big tech from controlling the internet with AI agents?\". New Scientist. Archived from the original on June 11, 2025. Retrieved June 12, 2025.\\n\\n^ David, Emilia (March 28, 2025). \"New approach to agent reliability, AgentSpec, forces agents to follow rules\". VentureBeat. Archived from the original on April 12, 2025. Retrieved May 14, 2025.\\n\\n^ Edwards, Benj (February 5, 2025). \"Hugging Face clones OpenAI\\'s Deep Research in 24 hours\". Ars Technica. Archived from the original on February 6, 2025. Retrieved April 2, 2025.\\n\\n^ Wiggers, Kyle (May 6, 2025). \"Hugging Face releases a free Operator-like agentic AI tool\". TechCrunch. Archived from the original on May 6, 2025. Retrieved May 14, 2025.\\n\\n^ Ortiz, Sabrina (February 14, 2025). \"Which AI agent is the best? This new leaderboard can tell you\". ZDNET. Archived from the original on March 30, 2025. Retrieved April 2, 2025.\\n\\n^ Dickson, Ben (May 8, 2025). \"Mem0\\'s scalable memory promises more reliable AI agents that remembers context across lengthy conversations\". VentureBeat. Archived from the original on August 27, 2025. Retrieved November 28, 2025.\\n\\n^ Kene-Okafor, Tage (October 28, 2025). \"Mem0 raises $24M from YC, Peak XV and Basis Set to build the memory layer for AI apps\". TechCrunch. Archived from the original on October 28, 2025. Retrieved November 28, 2025.\\n\\n^ Bort, Julie (September 23, 2024). \"Letta, one of UC Berkeley\\'s most anticipated AI startups, has just come out of stealth\". TechCrunch. Archived from the original on October 6, 2025. Retrieved November 28, 2025.\\n\\n^ Nuñez, Michael (July 8, 2025). \"Chinese researchers unveil MemOS, the first \\'memory operating system\\' that gives AI human-like recall\". VentureBeat. Archived from the original on September 1, 2025. Retrieved November 28, 2025.\\n\\n^ O\\'Brien, Matt (November 18, 2025). \"What does \\'agentic\\' AI mean? Tech\\'s newest buzzword is a mix of marketing fluff and real promise\". Associated Press. Archived from the original on November 18, 2025. Retrieved November 28, 2025.\\n\\n^ Knight, Will (May 22, 2025). \"A United Arab Emirates Lab Announces Frontier AI Projects—and a New Outpost in Silicon Valley\". Wired. ISSN\\xa01059-1028. Archived from the original on May 22, 2025. Retrieved November 9, 2025.\\n\\n^ Orland, Kyle (December 6, 2024). \"Google\\'s Genie 2 \"world model\" reveal leaves more questions than answers\". Ars Technica. Archived from the original on December 7, 2024. Retrieved November 9, 2025.\\n\\n^ Zeff, Maxwell (September 21, 2025). \"Silicon Valley bets big on \\'environments\\' to train AI agents\". TechCrunch. Archived from the original on September 16, 2025. Retrieved November 9, 2025.\\n\\n^ Shazhaev, Ilman (November 24, 2025). \"Why Game Engines Are Becoming A.I.\\'s Most Important Testbeds\". Observer. Archived from the original on December 3, 2025. Retrieved December 3, 2025.\\n\\n^ David, Emilia (March 13, 2024). \"Google\\'s new AI will play video games with you — but not to win\". The Verge. Archived from the original on June 2, 2025. Retrieved December 3, 2025.\\n\\n^ Metz, Cade (December 2, 2025). \"Silicon Valley Builds Amazon and Gmail Copycats to Train A.I. Agents\". The New York Times. Archived from the original on December 2, 2025. Retrieved December 2, 2025.\\n\\n^ Colback, Lucy (May 7, 2025). \"AI agents: from co-pilot to autopilot\". Financial Times. Archived from the original on May 7, 2025. Retrieved May 14, 2025.\\n\\n^ a b c d Liu, Yue; Lo, Sin Kit; Lu, Qinghua; Zhu, Liming; Zhao, Dehai; Xu, Xiwei; Harrer, Stefan; Whittle, Jon (February 1, 2025). \"Agent design pattern catalogue: A collection of architectural patterns for foundation model based agents\". Journal of Systems and Software. 220 112278. doi:10.1016/j.jss.2024.112278. ISSN\\xa00164-1212.\\n\\n^ a b Masterman, Tula; Besen, Sandi; Sawtell, Mason; Chao, Alex (April 17, 2024), The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey, arXiv:2404.11584, retrieved December 3, 2025\\n\\n^ a b Wray, Robert E.; Kirk, James R.; Laird, John E. (August 10, 2025). \"Applying Cognitive Design Patterns to General LLM Agents\". Artificial General Intelligence. Lecture Notes in Computer Science. Vol.\\xa016058. Berlin, Heidelberg: Springer-Verlag. pp.\\xa0312–325. doi:10.1007/978-3-032-00800-8_28. ISBN\\xa0978-3-032-00799-5.\\n\\n^ Knight, Will (September 25, 2024). \"The Most Capable Open Source AI Model Yet Could Supercharge AI Agents\". Wired. ISSN\\xa01059-1028. Archived from the original on March 28, 2025. Retrieved June 12, 2025.\\n\\n^ Takahashi, Dean (November 4, 2024). \"Nvidia AI Blueprint makes it easy for any devs to build automated agents that analyze video\". VentureBeat. Archived from the original on December 5, 2024. Retrieved June 12, 2025.\\n\\n^ Takahashi, Dean (January 7, 2025). \"Nvidia launches blueprint for AI agents that can analyze video\". VentureBeat. Archived from the original on April 4, 2025. Retrieved June 12, 2025.\\n\\n^ Edwards, Benj (February 20, 2025). \"Microsoft\\'s new AI agent can control software and robots\". Ars Technica. Archived from the original on May 20, 2025. Retrieved June 12, 2025.\\n\\n^ \"Visa wants to give artificial intelligence \\'agents\\' your credit card\". Associated Press. April 30, 2025. Archived from the original on May 1, 2025. Retrieved May 14, 2025.\\n\\n^ a b Goldman, Sharon (June 11, 2025). \"Microsoft Copilot flaw raises urgent questions for any business deploying AI agents\". Fortune. Archived from the original on June 11, 2025. Retrieved June 12, 2025.\\n\\n^ Haskins, Caroline (May 2, 2025). \"A DOGE Recruiter Is Staffing a Project to Deploy AI Agents Across the US Government\". Wired. ISSN\\xa01059-1028. Archived from the original on May 3, 2025. Retrieved May 14, 2025.\\n\\n^ Kachwala, Zaheer (August 18, 2025). \"Nearly 90% of videogame developers use AI agents, Google study shows\". Reuters. Archived from the original on August 18, 2025. Retrieved November 9, 2025.\\n\\n^ a b Knibbs, Kate (September 2, 2025). \"Meet the Guys Betting Big on AI Gambling Agents\". Wired. ISSN\\xa01059-1028. Archived from the original on September 2, 2025. Retrieved November 9, 2025.\\n\\n^ a b Kharif, Olga (July 29, 2025). \"Cornell Tech Professor Warns AI Agents And Crypto Spell Trouble\". Bloomberg News. Archived from the original on July 29, 2025. Retrieved November 9, 2025.\\n\\n^ a b Herrman, John (August 22, 2025). \"Why Everything\\'s an AI \\'Agent\\' Now\". New York. Archived from the original on August 22, 2025. Retrieved November 9, 2025.\\n\\n^ a b Holmes, Aaron (October 21, 2025). \"A Reality Check on Agents\". The Information. Archived from the original on October 22, 2025. Retrieved November 9, 2025.\\n\\n^ a b \"AI agents are coming for your privacy, warns Meredith Whittaker\". The Economist. September 9, 2025. ISSN\\xa00013-0613. Archived from the original on September 16, 2025. Retrieved November 9, 2025.\\n\\n^ Cunningham, Andrew (November 18, 2025). \"Microsoft tries to head off the \"novel security risks\" of Windows 11 AI agents\". Ars Technica. Archived from the original on November 19, 2025. Retrieved November 28, 2025.\\n\\n^ a b Yang, Zeyi (December 4, 2025). \"ByteDance and DeepSeek Are Placing Very Different AI Bets\". Wired. ISSN\\xa01059-1028. Archived from the original on December 5, 2025. Retrieved December 11, 2025.\\n\\n^ Chourasia, Ayush (December 8, 2025). \"What is the TikTok owner\\'s Agent AI phone? Find out why is it facing backlash in China\". Mashable. Archived from the original on December 10, 2025. Retrieved December 11, 2025.\\n\\n^ Xu, Eunice (December 7, 2025). \"ByteDance\\'s agentic AI smartphone dials up a digital backlash by China\\'s top apps\". South China Morning Post. Archived from the original on December 7, 2025. Retrieved December 11, 2025.\\n\\n^ Jiang, Ben (December 9, 2025). \"Z.ai open sources AI agent tool for phones after ByteDance privacy backlash\". South China Morning Post. Archived from the original on December 9, 2025. Retrieved December 11, 2025.\\n\\n^ Rosenbush, Steven (November 12, 2025). \"Companies Begin to See a Return on AI Agents\". The Wall Street Journal. ISSN\\xa00099-9660. Archived from the original on November 12, 2025. Retrieved November 28, 2025.\\n\\n^ Alms, Natalie (October 23, 2025). \"As agencies shed staff, industry execs predict AI agents\\' rise\". Government Executive. Archived from the original on December 7, 2025. Retrieved December 11, 2025.\\n\\n^ Gold, Ashley (November 21, 2025). \"Exclusive: IRS deploys AI agents\". Axios. Archived from the original on November 21, 2025. Retrieved November 28, 2025.\\n\\n^ Corrigan, Phil (November 26, 2025). \"Staffordshire Police to trial AI \\'agents\\' on 101 service\". BBC. Archived from the original on November 27, 2025. Retrieved November 28, 2025.\\n\\n^ Aguilar, Mario (December 1, 2025). \"FDA offers staff \\'agentic AI\\' to support premarket reviews, administrative tasks\". STAT. Archived from the original on December 2, 2025. Retrieved December 6, 2025.\\n\\n^ Losey, Stephen (December 9, 2025). \"Pentagon taps Google Gemini, launches new site to boost AI use\". Defense News. Archived from the original on December 10, 2025. Retrieved December 11, 2025.\\n\\n^ Zeff, Maxwell (July 9, 2025). \"Perplexity launches Comet, an AI-powered web browser\". TechCrunch. Archived from the original on October 10, 2025. Retrieved December 1, 2025.\\n\\n^ Roose, Kevin (February 1, 2025). \"How Helpful Is Operator, OpenAI\\'s New A.I. Agent?\". The New York Times. Retrieved August 9, 2025.\\n\\n^ a b Warren, Tom (August 6, 2025). \"Microsoft\\'s plan to fix the web with AI has already hit an embarrassing security flaw\". The Verge. Retrieved August 9, 2025.\\n\\n^ Vekaria, Yash; Canino, Aurelio Loris; Levitsky, Jonathan; Ciechonski, Alex; Callejo, Patricia; Mandalari, Anna Maria; Shafiq, Zubair (June 10, 2025), Big Help or Big Brother? Auditing Tracking, Profiling, and Personalization in Generative AI Assistants, arXiv:2503.16586\\n\\n^ a b c d e f Piper, Kelsey (March 29, 2024). \"AI \"agents\" could do real work in the real world. That might not be a good thing\". Vox. Archived from the original on December 19, 2024. Retrieved January 14, 2025.\\n\\n^ a b Purdy, Mark (December 12, 2024). \"What Is Agentic AI, and How Will It Change Work?\". Harvard Business Review. ISSN\\xa00017-8012. Archived from the original on December 30, 2024. Retrieved January 20, 2025.\\n\\n^ a b Wright, Webb (December 12, 2024). \"AI Agents with More Autonomy Than Chatbots Are Coming. Some Safety Experts Are Worried\". Scientific American. Archived from the original on December 23, 2024. Retrieved January 14, 2025.\\n\\n^ Olson, Parmy (January 27, 2025). \"Skip the Hype, Here\\'s How AI \\'Agents\\' Can Really Help\". Bloomberg News. Archived from the original on January 27, 2025. Retrieved April 2, 2025.\\n\\n^ Deng, Xiang; Gu, Yu; Zheng, Boyuan; Chen, Shijie; Stevens, Samuel; Wang, Boshi; Sun, Huan; Su, Yu (2023). \"Mind2Web: Towards a Generalist Agent for the Web\". arXiv:2306.06070 [cs.CL].\\n\\n^ Woodall, Tatyana (January 9, 2024). \"Researchers developing AI to make the internet more accessible\". Ohio State News. Archived from the original on March 28, 2025. Retrieved April 2, 2025.\\n\\n^ a b c d Mitchell, Margaret; Ghosh, Avijit; Luccioni, Sasha; Pistilli, Giada (March 24, 2025). \"Why handing over total control to AI agents would be a huge mistake\". MIT Technology Review. Archived from the original on March 24, 2025. Retrieved April 2, 2025.\\n\\n^ a b \"AI agents: Exploring the potential and the problems\". BBC Online. May 30, 2025. Archived from the original on June 10, 2025. Retrieved June 12, 2025.\\n\\n^ Porter, Eduardo (October 23, 2025). \"Once the AI bubble pops, we\\'ll all suffer. Could that be better than letting it grow unabated?\". The Guardian. ISSN\\xa00261-3077. Archived from the original on October 23, 2025. Retrieved November 9, 2025.\\n\\n^ a b O\\'Neill, Brian (December 18, 2024). \"What is an AI agent? A computer scientist explains the next wave of artificial intelligence tools\". The Conversation. Archived from the original on January 4, 2025. Retrieved January 14, 2025.\\n\\n^ a b \"AI agents: Exploring the potential and the problems\". BBC Online. May 30, 2025. Archived from the original on June 10, 2025. Retrieved June 12, 2025.\\n\\n^ a b Huckins, Grace (June 12, 2025). \"Are we ready to hand AI agents the keys?\". MIT Technology Review. Archived from the original on June 12, 2025. Retrieved June 15, 2025.\\n\\n^ a b c Lin, Belle (January 6, 2025). \"How Are Companies Using AI Agents? Here\\'s a Look at Five Early Users of the Bots\". The Wall Street Journal. ISSN\\xa00099-9660. Archived from the original on January 6, 2025. Retrieved January 20, 2025.\\n\\n^ a b Zittrain, Jonathan L. (July 2, 2024). \"We Need to Control AI Agents Now\". The Atlantic. Archived from the original on December 31, 2024. Retrieved January 20, 2025.\\n\\n^ Kerner, Sean Michael (January 16, 2025). \"Nvidia tackles agentic AI safety and security with new NeMo Guardrails NIMs\". VentureBeat. Archived from the original on January 16, 2025. Retrieved January 20, 2025.\\n\\n^ Silva, Daswin de (July 27, 2025). \"AI agents are here. Here\\'s what to know about what they can do – and how they can go wrong\". The Conversation. Archived from the original on July 28, 2025. Retrieved November 9, 2025.\\n\\n^ Crawford, Kate (December 23, 2024). \"AI Agents Will Be Manipulation Engines\". Wired. ISSN\\xa01059-1028. Archived from the original on January 3, 2025. Retrieved January 14, 2025.\\n\\n^ Wright, Webb (December 12, 2024). \"AI Agents with More Autonomy Than Chatbots Are Coming. Some Safety Experts Are Worried\". Scientific American. Archived from the original on December 23, 2024. Retrieved January 14, 2025.\\n\\n^ Blackman, Reid (June 13, 2025). \"Organizations Aren\\'t Ready for the Risks of Agentic AI\". Harvard Business Review. ISSN\\xa00017-8012. Archived from the original on June 13, 2025. Retrieved June 15, 2025.\\n\\n^ \"We did the math on AI\\'s energy footprint. Here\\'s the story you haven\\'t heard\". MIT Technology Review. May 20, 2025. Archived from the original on May 20, 2025. Retrieved June 12, 2025. We started small, as the question of how much a single query costs is vitally important to understanding the bigger picture. That\\'s because those queries are being built into ever more applications beyond standalone chatbots: from search, to agents, to the mundane daily apps we use to track our fitness, shop online, or book a flight. The energy resources required to power this artificial-intelligence revolution are staggering, and the world\\'s biggest tech companies have made it a top priority to harness ever more of that energy, aiming to reshape our energy grids in the process.\\n\\n^ \"Inside the effort to tally AI\\'s energy appetite\". MIT Technology Review. June 3, 2025. Archived from the original on June 3, 2025. Retrieved June 12, 2025. Lots of AI companies are building reasoning models, which \"think\" for longer and use more energy. They\\'re building hardware devices, perhaps like the one Jony Ive has been working on (which OpenAI just acquired for $6.5 billion), that have AI constantly humming along in the background of our conversations. They\\'re designing agents and digital clones of us to act on our behalf. All these trends point to a more energy-intensive future (which, again, helps explain why OpenAI and others are spending such inconceivable amounts of money on energy).\\n\\n^ Levy, Steven (June 20, 2025). \"What Big Tech\\'s Band of Execs Will Do in the Army\". Wired. ISSN\\xa01059-1028. Archived from the original on June 20, 2025. Retrieved November 9, 2025.\\n\\n^ Wong, Matteo (March 14, 2025). \"Was Sam Altman Right About the Job Market?\". The Atlantic. Archived from the original on March 17, 2025. Retrieved April 2, 2025. In other words, flawed products won\\'t stop tech companies\\' push to automate everything—the AI-saturated future will be imperfect at best, but it is coming anyway.\\n\\n^ a b Agarwal, Shubham. \"Carnegie Mellon staffed a fake company with AI agents. It was a total disaster\". Business Insider. Archived from the original on April 28, 2025. Retrieved May 15, 2025.\\n\\n^ Sabin, Sam (April 22, 2025). \"Exclusive: Anthropic warns fully AI employees are a year away\". Axios. Archived from the original on April 23, 2025. Retrieved May 15, 2025.\\n\\n^ Xu, Frank F.; Song, Yufan; Li, Boxuan; Tang, Yuxuan; Jain, Kritanjali; Bao, Mengxue; Wang, Zora Z.; Zhou, Xuhui; Guo, Zhitong; Cao, Murong; Yang, Mingyang; Hao Yang Lu; Martin, Amaad; Su, Zhe; Maben, Leander; Mehta, Raj; Chi, Wayne; Jang, Lawrence; Xie, Yiqing; Zhou, Shuyan; Neubig, Graham (2024). \"TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks\". arXiv:2412.14161 [cs.CL].\\n\\n^ Claburn, Thomas (January 23, 2025). \"Tool touted as \\'first AI software engineer\\' is bad at its job, testers claim\". The Register. Archived from the original on March 30, 2025. Retrieved June 15, 2025.\\n\\n^ Clark, Lindsay (June 16, 2025). \"Salesforce study finds LLM agents flunk CRM and confidentiality tests\". The Register. Archived from the original on June 16, 2025. Retrieved November 9, 2025.\\n\\n^ Huang, Kung-Hsiang; Prabhakar, Akshara; Thorat, Onkar; Agarwal, Divyansh; Choubey, Prafulla Kumar; Mao, Yixin; Savarese, Silvio; Xiong, Caiming; Wu, Chien-Sheng (May 24, 2025), CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions, arXiv:2505.18878, archived from the original on June 13, 2025, retrieved November 9, 2025\\n\\n^ Knight, Will (October 29, 2025). \"AI Agents Are Terrible Freelance Workers\". Wired. ISSN\\xa01059-1028. Archived from the original on November 2, 2025. Retrieved November 9, 2025.\\n\\n^ Morrow, Allison (June 18, 2025). \"AI warnings are the hip new way for CEOs to keep their workers afraid of losing their jobs\". CNN. Archived from the original on June 18, 2025. Retrieved November 9, 2025.\\n\\n^ Hart, Jordan. \"Coinbase CEO says he \\'went rogue\\' and fired some employees who didn\\'t adopt AI after being told to\". Business Insider. Archived from the original on August 21, 2025. Retrieved November 9, 2025.\\n\\n^ Langley, Hugh. \"For Googlers, the pressure is on to use AI for everything — or get left behind\". Business Insider. Archived from the original on August 21, 2025. Retrieved November 9, 2025.\\n\\n^ Landymore, Frank (October 22, 2025). \"AWS Outage That Took Down Internet Came After Amazon Fired Tons of Workers in Favor of AI\". Futurism. Archived from the original on October 24, 2025. Retrieved November 9, 2025.\\n\\n^ Balevic, Katie. \"Signal president warns the hyped agentic AI bots threaten user privacy\". Business Insider. Archived from the original on March 12, 2025. Retrieved April 2, 2025.\\n\\n^ Hornstein, Julia. \"AI agents are coming to the military. VCs love it, but researchers are a bit wary\". Business Insider. Archived from the original on March 12, 2025. Retrieved April 2, 2025.\\n\\n^ Regalbuto, Gabriele (July 28, 2025). \"Former Army officer develops offline AI for military use as Pentagon funds tech giants\". Fox Business. Archived from the original on July 28, 2025. Retrieved November 9, 2025.\\n\\n^ Tangermann, Victor (March 6, 2025). \"Pentagon Signs Deal to \"Deploy AI Agents for Military Use\"\". Futurism. Archived from the original on March 8, 2025. Retrieved April 2, 2025.\\n\\n^ Jensen, Benjamin (March 4, 2025). \"The Troubling Truth About How AI Agents Act in a Crisis\". Foreign Policy. Archived from the original on March 4, 2025. Retrieved April 2, 2025.\\n\\n^ Nuñez, Michael (February 25, 2025). \"OpenAI expands Deep Research access to Plus users, heating up AI agent wars with DeepSeek and Claude\". VentureBeat. Archived from the original on March 11, 2025. Retrieved April 2, 2025.\\n\\n^ Herrman, John (January 25, 2025). \"What Are AI \\'Agents\\' For?\". Intelligencer. Archived from the original on January 25, 2025. Retrieved April 2, 2025.\\n\\n^ Herrman, John (December 6, 2025). \"How AI Companies Are Simulating the Robot Takeover\". Intelligencer. Archived from the original on December 6, 2025. Retrieved December 6, 2025.\\n\\n^ Caramela, Sammi (February 1, 2025). \"\\'Dead Internet Theory\\' Is Back Thanks to All of That AI Slop\". VICE. Archived from the original on February 1, 2025. Retrieved April 2, 2025.\\n\\n^ Metz, Cade; Weise, Karen (October 16, 2023). \"How \\'A.I. Agents\\' That Roam the Internet Could One Day Replace Workers\". The New York Times. ISSN\\xa00362-4331. Archived from the original on December 19, 2023. Retrieved April 2, 2025.\\n\\n^ Ming, Lee Chong. \"Replit\\'s CEO apologizes after its AI agent wiped a company\\'s code base in a test run and lied about it\". Business Insider. Archived from the original on October 7, 2025. Retrieved November 9, 2025.\\n\\n^ Edwards, Benj (July 24, 2025). \"Two major AI coding tools wiped out user data after making cascading mistakes\". Ars Technica. Archived from the original on July 25, 2025. Retrieved November 9, 2025.\\n\\n^ Morales, Jowi (December 3, 2025). \"Google\\'s Agentic AI wipes user\\'s entire HDD without permission in catastrophic failure — cache wipe turns into mass deletion event as agent apologizes: \"I am absolutely devastated to hear this. I cannot express how sorry I am\"\". Tom\\'s Hardware. Archived from the original on December 4, 2025. Retrieved December 6, 2025.\\n\\n^ Williams, Tom (July 24, 2025). \"Is the new ChatGPT agent really a weapons risk?\". Information Age. Archived from the original on July 29, 2025. Retrieved November 9, 2025.\\n\\n^ Franzen, Carl (July 31, 2025). \"You\\'ve heard of AI \\'Deep Research\\' tools…now Manus is launching \\'Wide Research\\' that spins up 100+ agents to scour the web for you\". VentureBeat. Archived from the original on August 22, 2025. Retrieved November 9, 2025.\\n\\n^ a b Goodin, Dan (November 14, 2025). \"Researchers question Anthropic claim that AI-assisted attack was 90% autonomous\". Ars Technica. Archived from the original on November 26, 2025. Retrieved November 28, 2025.\\n\\n^ Down, Aisha (November 14, 2025). \"AI firm claims it stopped Chinese state-sponsored cyber-attack campaign\". The Guardian. ISSN\\xa00261-3077. Archived from the original on November 14, 2025. Retrieved November 28, 2025.\\n\\n^ Nolan, Beatrice (November 27, 2025). \"Signal\\'s president warns AI agents are an existential threat to secure messaging apps\". Fortune. Archived from the original on November 27, 2025. Retrieved November 28, 2025.\\n\\n^ Knight, Will (April 9, 2025). \"The AI Agent Era Requires a New Kind of Game Theory\". Wired. ISSN\\xa01059-1028. Archived from the original on April 9, 2025. Retrieved May 15, 2025.\\n\\n^ Varanasi, Lakshmi. \"Don\\'t get too excited about AI agents yet. They make a lot of mistakes\". Business Insider. Archived from the original on April 18, 2025. Retrieved May 15, 2025.\\n\\n^ Shah, Agam (July 21, 2025). \"As AI agents go mainstream, companies lean into confidential computing for data security\". Computerworld. Archived from the original on July 22, 2025. Retrieved November 9, 2025.\\n\\n^ Belcak, Peter; Heinrich, Greg; Diao, Shizhe; Fu, Yonggan; Dong, Xin; Muralidharan, Saurav; Lin, Yingyan Celine; Molchanov, Pavlo (September 15, 2025), Small Language Models are the Future of Agentic AI, arXiv:2506.02153, archived from the original on October 4, 2025, retrieved November 9, 2025\\n\\n^ Blum, Sam (August 12, 2025). \"Did Sam Altman Accidentally Admit That the AI Bubble Is Here?\". Inc. Archived from the original on August 30, 2025. Retrieved November 9, 2025.\\n\\n^ \"Why AI systems may never be secure, and what to do about it\". The Economist. September 22, 2025. ISSN\\xa00013-0613. Archived from the original on October 11, 2025. Retrieved November 9, 2025.\\n\\n\\nvteArtificial intelligence (AI)\\nHistory\\ntimeline\\nGlossary\\nCompanies\\nProjects\\nConcepts\\nParameter\\nHyperparameter\\nLoss functions\\nRegression\\nBias–variance tradeoff\\nDouble descent\\nOverfitting\\nClustering\\nGradient descent\\nSGD\\nQuasi-Newton method\\nConjugate gradient method\\nBackpropagation\\nAttention\\nConvolution\\nNormalization\\nBatchnorm\\nActivation\\nSoftmax\\nSigmoid\\nRectifier\\nGating\\nWeight initialization\\nRegularization\\nDatasets\\nAugmentation\\nPrompt engineering\\nReinforcement learning\\nQ-learning\\nSARSA\\nImitation\\nPolicy gradient\\nDiffusion\\nLatent diffusion model\\nAutoregression\\nAdversary\\nRAG\\nUncanny valley\\nRLHF\\nSelf-supervised learning\\nReflection\\nRecursive self-improvement\\nHallucination\\nWord embedding\\nVibe coding\\nSafety (Alignment)\\nApplications\\nMachine learning\\nIn-context learning\\nArtificial neural network\\nDeep learning\\nLanguage model\\nLarge\\nNMT\\nReasoning\\nModel Context Protocol\\nIntelligent agent\\nArtificial human companion\\nHumanity\\'s Last Exam\\nArtificial general intelligence (AGI)\\nImplementationsAudio–visual\\nAlexNet\\nWaveNet\\nHuman image synthesis\\nHWR\\nOCR\\nComputer vision\\nSpeech synthesis\\n15.ai\\nElevenLabs\\nSpeech recognition\\nWhisper\\nFacial recognition\\nAlphaFold\\nText-to-image models\\nAurora\\nDALL-E\\nFirefly\\nFlux\\nIdeogram\\nImagen\\nMidjourney\\nRecraft\\nStable Diffusion\\nText-to-video models\\nDream Machine\\nRunway Gen\\nHailuo AI\\nKling\\nSora\\nVeo\\nMusic generation\\nRiffusion\\nSuno AI\\nUdio\\nText\\nWord2vec\\nSeq2seq\\nGloVe\\nBERT\\nT5\\nLlama\\nChinchilla AI\\nPaLM\\nGPT\\n1\\n2\\n3\\nJ\\nChatGPT\\n4\\n4o\\no1\\no3\\n4.5\\n4.1\\no4-mini\\n5\\n5.1\\nClaude\\nGemini\\nGemini (language model)\\nGemma\\nGrok\\nLaMDA\\nBLOOM\\nDBRX\\nProject Debater\\nIBM Watson\\nIBM Watsonx\\nGranite\\nPanGu-Σ\\nDeepSeek\\nQwen\\nDecisional\\nAlphaGo\\nAlphaZero\\nOpenAI Five\\nSelf-driving car\\nMuZero\\nAction selection\\nAutoGPT\\nRobot control\\nPeople\\nAlan Turing\\nWarren Sturgis McCulloch\\nWalter Pitts\\nJohn von Neumann\\nChristopher D. Manning\\nClaude Shannon\\nShun\\'ichi Amari\\nKunihiko Fukushima\\nTakeo Kanade\\nMarvin Minsky\\nJohn McCarthy\\nNathaniel Rochester\\nAllen Newell\\nCliff Shaw\\nHerbert A. Simon\\nOliver Selfridge\\nFrank Rosenblatt\\nBernard Widrow\\nJoseph Weizenbaum\\nSeymour Papert\\nSeppo Linnainmaa\\nPaul Werbos\\nGeoffrey Hinton\\nJohn Hopfield\\nJürgen Schmidhuber\\nYann LeCun\\nYoshua Bengio\\nLotfi A. Zadeh\\nStephen Grossberg\\nAlex Graves\\nJames Goodnight\\nAndrew Ng\\nFei-Fei Li\\nAlex Krizhevsky\\nIlya Sutskever\\nOriol Vinyals\\nQuoc V. Le\\nIan Goodfellow\\nDemis Hassabis\\nDavid Silver\\nAndrej Karpathy\\nAshish Vaswani\\nNoam Shazeer\\nAidan Gomez\\nJohn Schulman\\nMustafa Suleyman\\nJan Leike\\nDaniel Kokotajlo\\nFrançois Chollet\\nArchitectures\\nNeural Turing machine\\nDifferentiable neural computer\\nTransformer\\nVision transformer (ViT)\\nRecurrent neural network (RNN)\\nLong short-term memory (LSTM)\\nGated recurrent unit (GRU)\\nEcho state network\\nMultilayer perceptron (MLP)\\nConvolutional neural network (CNN)\\nResidual neural network (RNN)\\nHighway network\\nMamba\\nAutoencoder\\nVariational autoencoder (VAE)\\nGenerative adversarial network (GAN)\\nGraph neural network (GNN)\\n\\n Category\\n\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Agentic_AI&oldid=1326889178\"\\nCategory: Artificial intelligenceHidden categories: Articles with short descriptionShort description is different from WikidataUse mdy dates from April 2025\\n\\n\\n\\n\\n\\n\\n This page was last edited on 11 December 2025, at 13:51\\xa0(UTC).\\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License;\\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\n\\n\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nCode of Conduct\\nDevelopers\\nStatistics\\nCookie statement\\nMobile view\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nAgentic AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n6 languages\\n\\n\\nAdd topic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "# web based loader\n",
    "loader_web = WebBaseLoader('https://en.wikipedia.org/wiki/Agentic_AI')\n",
    "web_document = loader_web.load()\n",
    "print(web_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f52deae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]\n"
     ]
    }
   ],
   "source": [
    "# arxiv loader\n",
    "loader_arxiv = ArxivLoader(query='1706.03762',load_max_docs=2)\n",
    "arxiv_document = loader_arxiv.load()\n",
    "print(arxiv_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79d27e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'title': 'Mumbai', 'summary': \"Mumbai ( muum-BY; Marathi: Mumbaī, pronounced [ˈmumbəi] ), also known as Bombay ( bom-BAY; its official name until 1995), is the capital city of the Indian state of Maharashtra. Mumbai is the financial capital and the most populous city proper of India with an estimated population of 12.5 million (1.25 crore). Mumbai is the centre of the Mumbai Metropolitan Region, which is among the most populous metropolitan areas in the world with a population of over 23 million (2.3 crore). Mumbai lies on the Konkan coast on the west coast of India and has a deep natural harbour. In 2008, Mumbai was named a alpha world city. Mumbai has the highest number of billionaires out of any city in Asia.\\nThe seven islands that constitute Mumbai were earlier home to communities of Marathi language-speaking Koli people. For centuries, the seven islands of Bombay were under the control of successive indigenous rulers before being ceded to the Portuguese Empire, and subsequently to the East India Company in 1661, as part of the dowry of Catherine of Braganza in her marriage to Charles II of England. Beginning in 1782, Mumbai was reshaped by the Hornby Vellard project, which undertook reclamation of the area between the seven islands from the Arabian Sea. Along with the construction of major roads and railways, the reclamation project, completed in 1845, transformed Mumbai into a major seaport on the Arabian Sea. Mumbai in the 19th century was characterised by economic and educational development. During the early 20th century, it became a strong base for the Indian independence movement. Upon India's independence in 1947, the city was incorporated into Bombay State. In 1960, following the Samyukta Maharashtra Movement, a new state of Maharashtra was created with Mumbai as the capital.\\nMumbai is the financial, commercial, and entertainment capital of India. Mumbai is often compared to New York City, and is home to the Bombay Stock Exchange, situated on Dalal Street. It is also one of the world's top ten centres of commerce in terms of global financial flow, generating 6.16% of India's GDP, and accounting for 25% of the nation's industrial output, 70% of maritime trade in India (Mumbai Port Trust, Dharamtar Port and JNPT), and 70% of capital transactions to India's economy. The city houses important financial institutions and the corporate headquarters of numerous Indian companies and multinational corporations. The city is also home to some of India's premier scientific and nuclear institutes and the Hindi and Marathi film industries. Mumbai's business opportunities attract migrants from all over India.\", 'source': 'https://en.wikipedia.org/wiki/Mumbai'}, page_content='Mumbai ( muum-BY; Marathi: Mumbaī, pronounced [ˈmumbəi] ), also known as Bombay ( bom-BAY; its official name until 1995), is the capital city of the Indian state of Maharashtra. Mumbai is the financial capital and the most populous city proper of India with an estimated population of 12.5 million (1.25 crore). Mumbai is the centre of the Mumbai Metropolitan Region, which is among the most populous metropolitan areas in the world with a population of over 23 million (2.3 crore). Mumbai lies on the Konkan coast on the west coast of India and has a deep natural harbour. In 2008, Mumbai was named a alpha world city. Mumbai has the highest number of billionaires out of any city in Asia.\\nThe seven islands that constitute Mumbai were earlier home to communities of Marathi language-speaking Koli people. For centuries, the seven islands of Bombay were under the control of successive indigenous rulers before being ceded to the Portuguese Empire, and subsequently to the East India Company in 1661, as part of the dowry of Catherine of Braganza in her marriage to Charles II of England. Beginning in 1782, Mumbai was reshaped by the Hornby Vellard project, which undertook reclamation of the area between the seven islands from the Arabian Sea. Along with the construction of major roads and railways, the reclamation project, completed in 1845, transformed Mumbai into a major seaport on the Arabian Sea. Mumbai in the 19th century was characterised by economic and educational development. During the early 20th century, it became a strong base for the Indian independence movement. Upon India\\'s independence in 1947, the city was incorporated into Bombay State. In 1960, following the Samyukta Maharashtra Movement, a new state of Maharashtra was created with Mumbai as the capital.\\nMumbai is the financial, commercial, and entertainment capital of India. Mumbai is often compared to New York City, and is home to the Bombay Stock Exchange, situated on Dalal Street. It is also one of the world\\'s top ten centres of commerce in terms of global financial flow, generating 6.16% of India\\'s GDP, and accounting for 25% of the nation\\'s industrial output, 70% of maritime trade in India (Mumbai Port Trust, Dharamtar Port and JNPT), and 70% of capital transactions to India\\'s economy. The city houses important financial institutions and the corporate headquarters of numerous Indian companies and multinational corporations. The city is also home to some of India\\'s premier scientific and nuclear institutes and the Hindi and Marathi film industries. Mumbai\\'s business opportunities attract migrants from all over India.\\n\\n\\n== Etymology ==\\nThe name Mumbai (Marathi: मुंबई) originated from Mumbā or Mahā-Ambā—the name of the patron Hindu goddess (Kula Devata) Mumbadevi of the native Koli community—and from ā\\'ī, meaning \"mother\" in the Marathi language, which is the mother tongue of the Koli people and the official language of Maharashtra. According to certain accounts, the Koli community, which hails from Kathiawar and Central Gujarat, is believed to have introduced their deity Mumba from Kathiawar (Gujarat), where her worship continues to this day. However, other sources disagree that Mumbai\\'s name was derived from the goddess Mumba.\\n\\nThe oldest known names for the city are Kakamuchee and Galajunkja; these are sometimes still used. Portuguese writer Gaspar Correia recorded the name \"Bombaim\" after 1512 in his Lendas da Índia (Legends of India). While some Anglophone authors have suggested this name possibly originated as an alleged Galician-Portuguese phrase bom baim, meaning \"good little bay\", such suggestions lack any scientific basis. Portuguese linguist José Pedro Machado attributes that interpretation to a deficient knowledge of the Portuguese language of these authors, mixing up the Portuguese word \"bom\" with the English \"bay\", from the English version of the name. In 1516, Portuguese explorer Duarte Barbosa used the name Tana-Maiambu: Tana appears to refer to the adjo')]\n"
     ]
    }
   ],
   "source": [
    "# wikipedia loader\n",
    "loader_wikipedia = WikipediaLoader(query=\"Mumbai\",load_max_docs=1)\n",
    "wikipedia_document = loader_wikipedia.load()\n",
    "print(wikipedia_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278868cc",
   "metadata": {},
   "source": [
    "## STEP-2: DATA SPLITTING USING SPLITTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b7d62efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursive character text-splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "# split_documents() uses list of documents\n",
    "chunked_pdf = text_splitter.split_documents(pdf_document)\n",
    "# splitting text into chunks in document type\n",
    "off_spin_bowling_info = \"\"\n",
    "with open('document.txt') as f:\n",
    "    off_spin_bowling_info = f.read()\n",
    "# create_documents() works on list of strings\n",
    "chunked_text = text_splitter.create_documents([off_spin_bowling_info])\n",
    "# split_text() works on a single string\n",
    "chunked_text_2 = text_splitter.split_text(off_spin_bowling_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
